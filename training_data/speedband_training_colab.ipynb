{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Speedband Prediction Model Training\n",
        "\n",
        "This notebook trains an XGBoost model to predict the next speedband value for each link in the traffic network.\n",
        "\n",
        "## Steps:\n",
        "1. Setup and Data Loading\n",
        "2. Data Analysis\n",
        "3. Data Preprocessing\n",
        "4. Feature Engineering\n",
        "5. Train/Validation/Test Split\n",
        "6. Model Training\n",
        "7. Model Evaluation\n",
        "8. Model Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install pandas numpy scikit-learn xgboost joblib pyarrow -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import Tuple, Dict, Any\n",
        "import joblib\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Upload Data File\n",
        "\n",
        "Upload the `correlated_traffic_data.parquet` file using the file uploader below, or mount Google Drive if your file is stored there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Upload file directly\n",
        "from google.colab import files\n",
        "\n",
        "# Uncomment the line below to upload file\n",
        "# uploaded = files.upload()\n",
        "# PARQUET_FILE = list(uploaded.keys())[0]\n",
        "\n",
        "# Option 2: Mount Google Drive (recommended for large files)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Update this path to your parquet file location in Google Drive\n",
        "PARQUET_FILE = '/content/drive/MyDrive/correlated_traffic_data.parquet'\n",
        "\n",
        "# Or if you uploaded directly, use:\n",
        "# PARQUET_FILE = '/content/correlated_traffic_data.parquet'\n",
        "\n",
        "print(f\"Parquet file path: {PARQUET_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load and Analyze Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_parquet(PARQUET_FILE)\n",
        "print(f\"Loaded {len(df):,} rows\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic data info\n",
        "print(\"=\" * 80)\n",
        "print(\"Data Overview\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "print(f\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "print(f\"\\nMissing values:\")\n",
        "print(df.isnull().sum())\n",
        "print(f\"\\nBasic statistics:\")\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time series analysis\n",
        "df['generated_at'] = pd.to_datetime(df['generated_at'])\n",
        "print(\"Time Series Analysis:\")\n",
        "print(f\"Date range: {df['generated_at'].min()} to {df['generated_at'].max()}\")\n",
        "print(f\"Total time span: {df['generated_at'].max() - df['generated_at'].min()}\")\n",
        "print(f\"Unique timestamps: {df['generated_at'].nunique()}\")\n",
        "print(f\"Average records per timestamp: {len(df) / df['generated_at'].nunique():.1f}\")\n",
        "print(f\"\\nUnique links: {df['LinkID'].nunique()}\")\n",
        "print(f\"Records per link: {df['LinkID'].value_counts().describe()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Speedband distribution\n",
        "print(\"Speedband Distribution:\")\n",
        "print(df['speedband'].value_counts().sort_index())\n",
        "print(f\"\\nSpeedband statistics:\")\n",
        "print(f\"Min: {df['speedband'].min()}, Max: {df['speedband'].max()}\")\n",
        "print(f\"Mean: {df['speedband'].mean():.2f}, Std: {df['speedband'].std():.2f}\")\n",
        "\n",
        "# Visualize speedband distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "df['speedband'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Speedband Distribution')\n",
        "plt.xlabel('Speedband')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if len(numeric_cols) > 1:\n",
        "    corr_matrix = df[numeric_cols].corr()\n",
        "    print(\"Correlation Matrix:\")\n",
        "    print(corr_matrix)\n",
        "    \n",
        "    # Visualize correlation\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0)\n",
        "    plt.title('Correlation Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Correlations with speedband\n",
        "    if 'speedband' in corr_matrix.columns:\n",
        "        print(\"\\nCorrelations with speedband:\")\n",
        "        speedband_corr = corr_matrix['speedband'].sort_values(ascending=False, key=abs)\n",
        "        for col, corr in speedband_corr.items():\n",
        "            if col != 'speedband':\n",
        "                print(f\"  {col}: {corr:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess data: convert timestamps, sort\n",
        "print(\"=\" * 80)\n",
        "print(\"Step 1: Data Preprocessing\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Convert generated_at to datetime (if not already)\n",
        "df['generated_at'] = pd.to_datetime(df['generated_at'])\n",
        "\n",
        "# Sort by LinkID and timestamp to ensure proper ordering\n",
        "print(\"Sorting by LinkID and timestamp...\")\n",
        "df = df.sort_values(['LinkID', 'generated_at']).reset_index(drop=True)\n",
        "\n",
        "print(f\"Data shape: {df.shape}\")\n",
        "print(f\"Date range: {df['generated_at'].min()} to {df['generated_at'].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create features\n",
        "print(\"=\" * 80)\n",
        "print(\"Step 2: Feature Engineering\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Time-based features\n",
        "print(\"Creating time-based features...\")\n",
        "df['hour'] = df['generated_at'].dt.hour\n",
        "df['minute'] = df['generated_at'].dt.minute\n",
        "\n",
        "print(\"Creating lag features and link-specific features...\")\n",
        "print(\"(This may take a while for large datasets...)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to create features for each link\n",
        "def create_link_features(group):\n",
        "    \"\"\"Create features for a single link.\"\"\"\n",
        "    group = group.sort_values('generated_at').reset_index(drop=True)\n",
        "    \n",
        "    # Lag features (PRIMARY FEATURES)\n",
        "    group['speedband_lag1'] = group['speedband'].shift(1)\n",
        "    group['speedband_lag2'] = group['speedband'].shift(2)\n",
        "    group['speedband_lag3'] = group['speedband'].shift(3)\n",
        "    group['speedband_lag5'] = group['speedband'].shift(5)\n",
        "    \n",
        "    # Rolling statistics over windows\n",
        "    for window in [3, 5, 10]:\n",
        "        group[f'speedband_rolling_mean_{window}'] = group['speedband'].shift(1).rolling(window=window, min_periods=1).mean()\n",
        "        group[f'speedband_rolling_std_{window}'] = group['speedband'].shift(1).rolling(window=window, min_periods=1).std().fillna(0)\n",
        "        group[f'speedband_rolling_min_{window}'] = group['speedband'].shift(1).rolling(window=window, min_periods=1).min()\n",
        "        group[f'speedband_rolling_max_{window}'] = group['speedband'].shift(1).rolling(window=window, min_periods=1).max()\n",
        "    \n",
        "    # Number of changes in rolling window\n",
        "    group['speedband_changes_3'] = (group['speedband'].shift(1).diff() != 0).rolling(window=3, min_periods=1).sum()\n",
        "    group['speedband_changes_5'] = (group['speedband'].shift(1).diff() != 0).rolling(window=5, min_periods=1).sum()\n",
        "    \n",
        "    # Speedband change rate\n",
        "    group['speedband_diff'] = group['speedband'].shift(1).diff().fillna(0)\n",
        "    \n",
        "    # Link-specific features\n",
        "    # Historical average (using all previous data)\n",
        "    group['link_avg_speedband'] = group['speedband'].shift(1).expanding().mean()\n",
        "    group['link_std_speedband'] = group['speedband'].shift(1).expanding().std().fillna(0)\n",
        "    \n",
        "    # Rolling average of rainfall\n",
        "    group['rainfall_rolling_mean_3'] = group['rainfall_mm'].shift(1).rolling(window=3, min_periods=1).mean()\n",
        "    group['rainfall_rolling_mean_5'] = group['rainfall_mm'].shift(1).rolling(window=5, min_periods=1).mean()\n",
        "    \n",
        "    # Target variable: next speedband value\n",
        "    group['target'] = group['speedband'].shift(-1)\n",
        "    \n",
        "    return group\n",
        "\n",
        "# Apply feature engineering to all links\n",
        "print(\"Processing links (this may take a while)...\")\n",
        "df = df.groupby('LinkID', group_keys=False).apply(create_link_features).reset_index(drop=True)\n",
        "print(\"Feature engineering complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill NaN values in lag features (first few rows per link)\n",
        "lag_cols = [col for col in df.columns if 'lag' in col or 'rolling' in col or 'link_' in col]\n",
        "for col in lag_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].fillna(df[col].median() if df[col].dtype in [np.float64, np.int64] else 0)\n",
        "\n",
        "# Encode LinkID as categorical (using numeric encoding for XGBoost)\n",
        "print(\"Encoding LinkID...\")\n",
        "df['LinkID_encoded'] = pd.Categorical(df['LinkID']).codes\n",
        "\n",
        "# Convert boolean to int\n",
        "df['has_incident'] = df['has_incident'].astype(int)\n",
        "\n",
        "# Drop rows where target is NaN (last row of each link)\n",
        "print(\"Dropping rows with missing target...\")\n",
        "initial_rows = len(df)\n",
        "df = df.dropna(subset=['target']).reset_index(drop=True)\n",
        "print(f\"Dropped {initial_rows - len(df):,} rows with missing target\")\n",
        "\n",
        "print(f\"Final feature matrix shape: {df.shape}\")\n",
        "feature_cols = [c for c in df.columns if c not in ['LinkID', 'generated_at', 'speedband', 'target']]\n",
        "print(f\"Features created: {len(feature_cols)}\")\n",
        "print(f\"Feature columns: {feature_cols[:10]}...\")  # Show first 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Train/Validation/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into train/validation/test sets using time-based split\n",
        "print(\"=\" * 80)\n",
        "print(\"Step 3: Train/Validation/Test Split\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "train_dfs = []\n",
        "val_dfs = []\n",
        "test_dfs = []\n",
        "\n",
        "print(\"Splitting by link (time-based split)...\")\n",
        "for link_id, group in df.groupby('LinkID'):\n",
        "    group = group.sort_values('generated_at').reset_index(drop=True)\n",
        "    n = len(group)\n",
        "    \n",
        "    # Split indices: 70% train, 15% val, 15% test\n",
        "    train_end = int(n * 0.70)\n",
        "    val_end = int(n * 0.85)\n",
        "    \n",
        "    train_dfs.append(group.iloc[:train_end])\n",
        "    val_dfs.append(group.iloc[train_end:val_end])\n",
        "    test_dfs.append(group.iloc[val_end:])\n",
        "\n",
        "train_df = pd.concat(train_dfs, ignore_index=True)\n",
        "val_df = pd.concat(val_dfs, ignore_index=True)\n",
        "test_df = pd.concat(test_dfs, ignore_index=True)\n",
        "\n",
        "print(f\"Training set: {len(train_df):,} rows ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Validation set: {len(val_df):,} rows ({len(val_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Test set: {len(test_df):,} rows ({len(test_df)/len(df)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare feature matrices\n",
        "exclude_cols = ['LinkID', 'generated_at', 'speedband', 'target']\n",
        "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "X_train = train_df[feature_cols]\n",
        "y_train = train_df['target']\n",
        "X_val = val_df[feature_cols]\n",
        "y_val = val_df['target']\n",
        "X_test = test_df[feature_cols]\n",
        "y_test = test_df['target']\n",
        "\n",
        "print(f\"Feature matrix shape: {X_train.shape}\")\n",
        "print(f\"Number of features: {len(feature_cols)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train XGBoost model\n",
        "print(\"=\" * 80)\n",
        "print(\"Step 4: Model Training\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"Training XGBoost regressor...\")\n",
        "\n",
        "# XGBoost parameters\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 500,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'min_child_weight': 3,\n",
        "    'gamma': 0.1,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "}\n",
        "\n",
        "model = xgb.XGBRegressor(**params)\n",
        "\n",
        "# Train with early stopping\n",
        "print(\"Training with early stopping on validation set...\")\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    early_stopping_rounds=20,\n",
        "    verbose=100\n",
        ")\n",
        "\n",
        "print(f\"\\nBest iteration: {model.best_iteration}\")\n",
        "print(f\"Best score: {model.best_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "print(\"=\" * 80)\n",
        "print(\"Step 6: Model Evaluation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def evaluate_model(model, X, y, dataset_name):\n",
        "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
        "    print(f\"\\nEvaluating on {dataset_name} set...\")\n",
        "    \n",
        "    # Predict\n",
        "    y_pred = model.predict(X)\n",
        "    \n",
        "    # Clip predictions to valid range [0, 8]\n",
        "    y_pred = np.clip(y_pred, 0, 8)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    mae = mean_absolute_error(y, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
        "    mape = np.mean(np.abs((y - y_pred) / (y + 1e-8))) * 100\n",
        "    y_pred_rounded = np.round(y_pred).astype(int)\n",
        "    accuracy = (y_pred_rounded == y).mean() * 100\n",
        "    \n",
        "    metrics = {\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'Accuracy': accuracy\n",
        "    }\n",
        "    \n",
        "    print(f\"  MAE: {mae:.4f}\")\n",
        "    print(f\"  RMSE: {rmse:.4f}\")\n",
        "    print(f\"  MAPE: {mape:.2f}%\")\n",
        "    print(f\"  Accuracy (exact match): {accuracy:.2f}%\")\n",
        "    \n",
        "    return metrics, y_pred\n",
        "\n",
        "# Evaluate on all sets\n",
        "train_metrics, y_train_pred = evaluate_model(model, X_train, y_train, \"Training\")\n",
        "val_metrics, y_val_pred = evaluate_model(model, X_val, y_val, \"Validation\")\n",
        "test_metrics, y_test_pred = evaluate_model(model, X_test, y_test, \"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions vs actual\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, (name, y_true, y_pred) in enumerate([\n",
        "    ('Training', y_train, y_train_pred),\n",
        "    ('Validation', y_val, y_val_pred),\n",
        "    ('Test', y_test, y_test_pred)\n",
        "]):\n",
        "    axes[idx].scatter(y_true, y_pred, alpha=0.1, s=1)\n",
        "    axes[idx].plot([0, 8], [0, 8], 'r--', lw=2)\n",
        "    axes[idx].set_xlabel('Actual Speedband')\n",
        "    axes[idx].set_ylabel('Predicted Speedband')\n",
        "    axes[idx].set_title(f'{name} Set Predictions')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance\n",
        "print(\"\\nTop 20 Most Important Features:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "importance = model.feature_importances_\n",
        "indices = np.argsort(importance)[::-1][:20]\n",
        "\n",
        "for i, idx in enumerate(indices, 1):\n",
        "    print(f\"{i:2d}. {feature_cols[idx]:40s} {importance[idx]:.4f}\")\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_features = [feature_cols[idx] for idx in indices]\n",
        "top_importance = [importance[idx] for idx in indices]\n",
        "plt.barh(range(len(top_features)), top_importance)\n",
        "plt.yticks(range(len(top_features)), top_features)\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Top 20 Feature Importances')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model\n",
        "print(\"=\" * 80)\n",
        "print(\"Step 5: Saving Model\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create models directory\n",
        "MODEL_DIR = '/content/models'\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_FILE = os.path.join(MODEL_DIR, 'speedband_model.joblib')\n",
        "FEATURE_NAMES_FILE = os.path.join(MODEL_DIR, 'feature_names.txt')\n",
        "\n",
        "print(f\"Saving model to {MODEL_FILE}...\")\n",
        "joblib.dump(model, MODEL_FILE)\n",
        "\n",
        "print(f\"Saving feature names to {FEATURE_NAMES_FILE}...\")\n",
        "with open(FEATURE_NAMES_FILE, 'w') as f:\n",
        "    for name in feature_cols:\n",
        "        f.write(f\"{name}\\n\")\n",
        "\n",
        "print(\"Model saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download model files (optional - to save to local machine)\n",
        "from google.colab import files\n",
        "\n",
        "# Download model\n",
        "files.download(MODEL_FILE)\n",
        "\n",
        "# Download feature names\n",
        "files.download(FEATURE_NAMES_FILE)\n",
        "\n",
        "print(\"Files downloaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print final summary\n",
        "print(\"=\" * 80)\n",
        "print(\"Training Summary\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Training MAE: {train_metrics['MAE']:.4f}, Accuracy: {train_metrics['Accuracy']:.2f}%\")\n",
        "print(f\"Validation MAE: {val_metrics['MAE']:.4f}, Accuracy: {val_metrics['Accuracy']:.2f}%\")\n",
        "print(f\"Test MAE: {test_metrics['MAE']:.4f}, Accuracy: {test_metrics['Accuracy']:.2f}%\")\n",
        "print(f\"\\nModel saved to: {MODEL_FILE}\")\n",
        "print(f\"End time: {datetime.now()}\")\n",
        "print(\"=\" * 80)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
